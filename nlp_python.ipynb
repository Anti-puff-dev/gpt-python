{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv4aqCa0bw8iG4zyOoHfsb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anti-puff-dev/gpt-python/blob/main/nlp_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELS"
      ],
      "metadata": {
        "id": "tQfYxaCqTezO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "metadata": {
        "id": "FyDp2SzitP4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.19.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xwmrkzRh-WR",
        "outputId": "73fd9ca5-d40e-4a6b-c812-9e3248427179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf==3.19.0 in /usr/local/lib/python3.11/dist-packages (3.19.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Iterable, Optional\n",
        "import re\n",
        "import math\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "z6YdnfbLhX8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq8r91T-Sp_R"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, word: str, word_partial: str, count: int):\n",
        "        self.word = word\n",
        "        self.word_partial = word_partial\n",
        "        self.count = count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Token:\n",
        "    def __init__(self, index: int, count: int, position: List[int], weight: float, nonce: bool):\n",
        "        self.index = index\n",
        "        self.count = count\n",
        "        self.position = position\n",
        "        self.weight = weight\n",
        "        self.nonce = nonce"
      ],
      "metadata": {
        "id": "kstUy2Y_TKMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tensor:\n",
        "    def __init__(self, category_id, word_id, weight):\n",
        "        self.category_id = category_id\n",
        "        self.word_id = word_id\n",
        "        self.weight = weight"
      ],
      "metadata": {
        "id": "c0_V2AePTLqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenVec:\n",
        "    def __init__(self, index: int, count: int, weight: float):\n",
        "        self.index = index\n",
        "        self.count = count\n",
        "        self.weight = weight"
      ],
      "metadata": {
        "id": "XfdsCv6VTQBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity:\n",
        "    def __init__(self, c1, c2, value):\n",
        "        self.c1 = c1\n",
        "        self.c2 = c2\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "Sag2_Z56TTsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "    def __init__(self, values = None):\n",
        "        self.values = values if values is not None else []\n",
        "\n",
        "class WordVector:\n",
        "    def __init__(self, embeddings = None):\n",
        "        self.embeddings = embeddings if embeddings is not None else []"
      ],
      "metadata": {
        "id": "BAyNlpAVTYBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASSES"
      ],
      "metadata": {
        "id": "8dQZSrYcTl7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenize:\n",
        "    def __init__(self, word_pooling: float = 1.0):\n",
        "        self.word_pooling = word_pooling\n",
        "\n",
        "    @staticmethod\n",
        "    def instance(word_pooling: float = 1.0) -> 'Tokenize':\n",
        "        return Tokenize(word_pooling)\n",
        "\n",
        "    def add_endings(self, lists: List[List[str]]) -> List[List[str]]:\n",
        "        for i in range(len(lists)):\n",
        "            for j in range(len(lists[i])):\n",
        "                lists[i][j] += \" <end>\"\n",
        "        return lists\n",
        "\n",
        "    def vocab(self, lists: List[List[str]]) -> List[Vocab]:\n",
        "        lists = self.add_endings(lists)\n",
        "        vocab_dict = {}\n",
        "\n",
        "        for lst in lists:\n",
        "            for sentence in lst:\n",
        "                words = self.pontuation_v(sentence).lower().split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dict:\n",
        "                        vocab_dict[word] += 1\n",
        "                    else:\n",
        "                        vocab_dict[word] = 1\n",
        "\n",
        "        return sorted(\n",
        "            [\n",
        "                Vocab(word, self.word_pooling_func(word, self.word_pooling), count)\n",
        "                for word, count in vocab_dict.items()\n",
        "            ],\n",
        "            key=lambda v: v.word,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def embedding(self, category_id: int, text: str, vocab: List[Vocab]) -> List[int]:\n",
        "        embed = [category_id]\n",
        "        words = self.pontuation(text + \" <end>\").lower().split()\n",
        "\n",
        "        for word in words:\n",
        "            index = next((i for i, v in enumerate(vocab) if v.word == word), -1)\n",
        "            if index != -1:\n",
        "                embed.append(index)\n",
        "\n",
        "        return embed\n",
        "\n",
        "    def apply(self, text: str, vocab: List[Vocab]) -> Tuple[List[Token], int]:\n",
        "        tokens = []\n",
        "        lst = self.pontuation(text + \" <end>\").split()\n",
        "        result = {word: len(list(group)) for word, group in itertools.groupby(sorted(lst))}\n",
        "\n",
        "        for word, count in result.items():\n",
        "            word_index = next((i for i, v in enumerate(vocab) if v.word_partial == self.word_pooling_func(word, self.word_pooling)), -1)\n",
        "            if word_index != -1:\n",
        "                ids = self.get_ids(lst, word)\n",
        "                is_nonce = self.is_nonce(word)\n",
        "                tokens.append(Token(word_index, count, list(ids), 0 if is_nonce else 1, is_nonce))\n",
        "\n",
        "        return tokens, len(lst)\n",
        "\n",
        "    def pontuation(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        for char in [\",\", \".\", \"*\", \":\", \"'\"]:\n",
        "            text = text.replace(char, f\" {char} \")\n",
        "        for char in [\"?\", \"!\"]:\n",
        "            text = text.replace(char, \"\")\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def pontuation_v(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        for char in [\",\", \".\", \"*\", \":\", \"'\"]:\n",
        "            text = text.replace(char, f\" {char} \")\n",
        "        for char in [\"?\", \"!\"]:\n",
        "            text = text.replace(char, \" \")\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    @staticmethod\n",
        "    def word_pooling_func(word: str, rate: float) -> str:\n",
        "        length = len(word)\n",
        "        if length <= 4:\n",
        "            return word\n",
        "        pad = math.ceil(length * rate)\n",
        "        return word[:pad]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_ids(lst: List[str], word: str) -> Iterable[int]:\n",
        "        return [i for i, w in enumerate(lst) if w == word]\n",
        "\n",
        "    @staticmethod\n",
        "    def is_nonce(word: str) -> bool:\n",
        "        nonce_words = {\n",
        "            \"das\", \"da\", \"de\", \"do\", \"dos\", \"na\", \"nas\", \"no\", \"nos\", \"em\", \"o\", \"a\", \"as\", \"os\",\n",
        "            \"e\", \"com\", \"que\", \"ao\", \"aos\", \" , \", \" . \", \" : \", \" ? \", \" ! \", \" ' \", \" * \", \",\",\n",
        "            \".\", \":\", \"?\", \"!\", \"'\", \"*\", \"<end>\"\n",
        "        }\n",
        "        return word in nonce_words\n",
        "\n",
        "    def print_vocab(self, vocab: List[Vocab]):\n",
        "        for item in vocab:\n",
        "            print(f\"{item.word} {item.word_partial} {item.count}\")\n",
        "\n",
        "    def print_tokens(self, tokens: List[Token], vocab: List[Vocab]):\n",
        "        for item in tokens:\n",
        "            print(f\"{item.index} {item.count} {vocab[item.index].word}\")\n",
        "\n",
        "    def print_ids(self, ids: Iterable[int]):\n",
        "        for item in ids:\n",
        "            print(item)"
      ],
      "metadata": {
        "id": "lGUdTw2bayCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategorySmall:\n",
        "    def __init__(self, id: int, name: str):\n",
        "        self.id = id\n",
        "        self.name = name\n",
        "\n",
        "class Category:\n",
        "    def __init__(self, id: int, name: str):\n",
        "        self.id = id\n",
        "        self.name = name\n",
        "        self.tokens: List[Token] = []\n",
        "        self._len = 0\n",
        "\n",
        "    def add_tokens(self, data: Tuple[List[Token], int]):\n",
        "        tokens, length = data\n",
        "\n",
        "        for token in tokens:\n",
        "            index = next((i for i, t in enumerate(self.tokens) if t.index == token.index), -1)\n",
        "\n",
        "            for i in range(len(token.position)):\n",
        "                token.position[i] += self._len\n",
        "\n",
        "            if index == -1:\n",
        "                self.tokens.append(Token(token.index, token.count, token.position, token.weight, token.nonce))\n",
        "            else:\n",
        "                self.tokens[index].count += token.count\n",
        "                self.tokens[index].position.extend(token.position)\n",
        "\n",
        "        self._len += length\n",
        "\n",
        "\n",
        "    def test(self, word: str, next_word: str, vocab: List['Vocab']):\n",
        "        index0 = next((i for i, v in enumerate(vocab) if v.word == word), -1)\n",
        "        index1 = next((i for i, v in enumerate(vocab) if v.word == next_word), -1)\n",
        "\n",
        "        t1 = next((t for t in self.tokens if t.index == index0), None)\n",
        "        t2 = next((t for t in self.tokens if t.index == index1), None)\n",
        "\n",
        "        if index0 > -1 and index1 > -1 and t1 is not None and t2 is not None:\n",
        "            near_position = self.alpha_minor_distance_position(t1.position, t2.position)\n",
        "            print(f\"{word} {next_word}| {near_position}\")\n",
        "\n",
        "        print(f\"{word} {next_word}| {1000}\")\n",
        "\n",
        "    def alpha_minor_distance_position(self, position: List[int], ids: List[int]) -> int:\n",
        "        dist = 10000\n",
        "        rid = 0\n",
        "\n",
        "        for pos in position:\n",
        "            for id in ids:\n",
        "                dst = abs(pos - id)\n",
        "                if dst < dist:\n",
        "                    dist = dst\n",
        "                    rid = id\n",
        "\n",
        "        return dist\n",
        "\n",
        "    @staticmethod\n",
        "    def multiply(vector: List[float], matrix: List[List[float]]) -> List[float]:\n",
        "        return [sum(vector[j] * matrix[j][i] for j in range(len(vector))) for i in range(len(matrix[0]))]\n",
        "\n",
        "    @staticmethod\n",
        "    def dot(vector1: List[float], vector2: List[float]) -> float:\n",
        "        return sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(vector: List[float]) -> List[float]:\n",
        "        exp_values = [math.exp(v) for v in vector]\n",
        "        sum_exp = sum(exp_values)\n",
        "        return [v / sum_exp for v in exp_values]\n",
        "\n",
        "    @staticmethod\n",
        "    def multiply_scalar(scalar: float, vector: List[float]) -> List[float]:\n",
        "        return [scalar * v for v in vector]\n",
        "\n",
        "    @staticmethod\n",
        "    def sum(vectors: List[List[float]]) -> List[float]:\n",
        "        return [sum(vector[i] for vector in vectors) for i in range(len(vectors[0]))]"
      ],
      "metadata": {
        "id": "0O8_5aGla7Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, tensors: List[Tensor] = None, vocab: List[Vocab] = None, categories: List[CategorySmall] = None, vectors: WordVector = None):\n",
        "        self.tensors = tensors or []\n",
        "        self.vocab = vocab or []\n",
        "        self.categories =  categories or []\n",
        "        self.vectors = vectors or WordVector()"
      ],
      "metadata": {
        "id": "itWErxrshmb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!protoc --python_out=. modelproto.proto"
      ],
      "metadata": {
        "id": "cImwFKhEsYbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content\")"
      ],
      "metadata": {
        "id": "liocVJLws8RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import modelproto_pb2"
      ],
      "metadata": {
        "id": "Umm37icmttTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProtobufConverter:\n",
        "    @staticmethod\n",
        "    def to_proto(model: Model) -> modelproto_pb2.ModelProto:\n",
        "        proto_model = modelproto_pb2.ModelProto()\n",
        "\n",
        "        # Converter tensors\n",
        "        for tensor in model.tensors:\n",
        "            proto_tensor = proto_model.tensors.add()\n",
        "            proto_tensor.category_id = tensor.category_id\n",
        "            proto_tensor.word_id = tensor.word_id\n",
        "            proto_tensor.weight = tensor.weight\n",
        "\n",
        "        # Converter vocab\n",
        "        for vocab in model.vocab:\n",
        "            proto_vocab = proto_model.vocab.add()\n",
        "            proto_vocab.word = vocab.word\n",
        "            proto_vocab.word_partial = vocab.word_partial\n",
        "\n",
        "        # Converter categories\n",
        "        for category in model.categories:\n",
        "            proto_category = proto_model.categories.add()\n",
        "            if isinstance(category, dict):\n",
        "                proto_category.id = category[\"id\"]\n",
        "                proto_category.name = category[\"name\"]\n",
        "            else:  # Handle CategorySmall objects\n",
        "                proto_category.id = category.id\n",
        "                proto_category.name = category.name\n",
        "\n",
        "        # Converter vectors\n",
        "        #proto_model.vectors = modelproto_pb2.WordVectorProto()\n",
        "        if isinstance(model.vectors, dict):  # Handle dictionaries\n",
        "            for emb in model.vectors[\"embeddings\"]:\n",
        "                proto_emb = proto_model.vectors.embeddings.add()\n",
        "                proto_emb.values.extend(emb[\"values\"])\n",
        "        else:  # Handle WordVector objects\n",
        "            for emb in model.vectors.embeddings:\n",
        "                proto_emb = proto_model.vectors.embeddings.add()\n",
        "                proto_emb.values.extend(emb.values)\n",
        "\n",
        "        return proto_model\n",
        "\n",
        "    @staticmethod\n",
        "    def from_proto(proto_model: modelproto_pb2.ModelProto) -> Model:\n",
        "        model = Model()\n",
        "\n",
        "        # Converter tensors\n",
        "        for proto_tensor in proto_model.tensors:\n",
        "            tensor = Tensor(\n",
        "                category_id=proto_tensor.category_id,\n",
        "                word_id=proto_tensor.word_id,\n",
        "                weight=proto_tensor.weight\n",
        "            )\n",
        "            model.tensors.append(tensor)\n",
        "\n",
        "        # Converter vocab\n",
        "        for proto_vocab in proto_model.vocab:\n",
        "            vocab = Vocab(\n",
        "                word=proto_vocab.word,\n",
        "                word_partial=proto_vocab.word_partial,\n",
        "                count=proto_vocab.count\n",
        "            )\n",
        "            model.vocab.append(vocab)\n",
        "\n",
        "        # Converter categories\n",
        "        for proto_category in proto_model.categories:\n",
        "            category = CategorySmall(\n",
        "                id=proto_category.id,\n",
        "                name=proto_category.name\n",
        "            )\n",
        "            model.categories.append(category)\n",
        "\n",
        "        # Converter vectors\n",
        "        model.vectors = WordVector()\n",
        "        for proto_emb in proto_model.vectors.embeddings:\n",
        "            emb = Embedding(values=list(proto_emb.values))\n",
        "            model.vectors.embeddings.append(emb)\n",
        "\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def save_to_file(model: Model, file_path: str):\n",
        "        proto_model = ProtobufConverter.to_proto(model)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(proto_model.SerializeToString())\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_file(file_path: str) -> Model:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            proto_model = modelproto_pb2.ModelProto()\n",
        "            proto_model.ParseFromString(f.read())\n",
        "            return ProtobufConverter.from_proto(proto_model)"
      ],
      "metadata": {
        "id": "K2rcj-mtqg2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Classify:\n",
        "    def __init__(self, model_path=None, vocab=None, categories=None, tensors=None, vectors=None):\n",
        "        self.word_pooling  = 1.0\n",
        "        self.dropout = 0.0001\n",
        "        self.classes_number = 3\n",
        "        self.non_category_threshold = 0.05\n",
        "        self.distance_temperature = 1.0\n",
        "\n",
        "        self.categories = categories or []\n",
        "        self.tensors = tensors or []\n",
        "        if vocab != None: self.vocab = vocab or []\n",
        "        self.vectors = vectors or []\n",
        "        self.model_path = model_path or \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def instance(model_path=None, vocab=None, categories=None, tensors=None, vectors=None):\n",
        "        return Classify(model_path, vocab, categories, tensors, vectors)\n",
        "\n",
        "    def word_pooling(self, word_pooling):\n",
        "        self.word_pooling = word_pooling\n",
        "        return self\n",
        "\n",
        "    def mode(self, mode):\n",
        "        self.mode = mode\n",
        "        return self\n",
        "\n",
        "    def vocab(self, vocab: List[Vocab]):\n",
        "        self.vocab = vocab\n",
        "        return self\n",
        "\n",
        "    def distance_temperature(self, distance_temperature):\n",
        "        self.distance_temperature = distance_temperature\n",
        "        return self\n",
        "\n",
        "    def model(self, model_path, read=False):\n",
        "        self.model_path = model_path\n",
        "        if read and self._file_exists(model_path):\n",
        "            self.tensors, self.vocab, _categories, self.vectors = self.load_model(model_path)\n",
        "            self.categories = self._fit_categories_from_small(_categories)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def _file_exists(self, file_path: str) -> bool:\n",
        "        return os.path.isfile(file_path)\n",
        "\n",
        "    def dropout(self, dropout):\n",
        "        self.dropout = dropout\n",
        "        return self\n",
        "\n",
        "    def classes_number(self, classes_number):\n",
        "        self.classes_number = classes_number\n",
        "        return self\n",
        "\n",
        "    def non_category_threshold(self, nct):\n",
        "        self.non_category_threshold = nct\n",
        "        return self\n",
        "\n",
        "    def add_category(self, name, lists):\n",
        "      if isinstance(lists[0], list):\n",
        "        for lst in lists:\n",
        "            for text in lst:\n",
        "                self._add_category(name, text)\n",
        "      else:\n",
        "        for text in lists:\n",
        "            self._add_category(name, text)\n",
        "      return self\n",
        "\n",
        "    def _add_category(self, name, text):\n",
        "        index = next((i for i, c in enumerate(self.categories) if c.name == name), -1)\n",
        "        if index == -1:\n",
        "            category_id = len(self.categories) + 1\n",
        "            category = Category(id=category_id, name=name)\n",
        "            category.add_tokens(Tokenize.instance(self.word_pooling).apply(text, self.vocab))\n",
        "            self.categories.append(category)\n",
        "            ids = Tokenize(self.word_pooling).embedding(category_id, text, self.vocab)\n",
        "            self.vectors.append(ids)\n",
        "        else:\n",
        "            self.categories[index].add_tokens(Tokenize(self.word_pooling).apply(text, self.vocab))\n",
        "            ids = Tokenize(self.word_pooling).embedding(self.categories[index].id, text, self.vocab)\n",
        "            self.vectors.append(ids)\n",
        "        return self\n",
        "\n",
        "    def auto_categorize(self, lists):\n",
        "        flat_list = []\n",
        "        if isinstance(lists[0], list):\n",
        "          for lst in lists:\n",
        "              flat_list.extend(lst)\n",
        "        else:\n",
        "          flat_list = lists\n",
        "\n",
        "        sims = []\n",
        "        for i in range(len(flat_list) - 1):\n",
        "            for j in range(i + 1, len(flat_list)):\n",
        "                similarity = self._similarity(flat_list[i], flat_list[j])\n",
        "                sims.append(Similarity(c1=i, c2=j, value=similarity))\n",
        "\n",
        "        classes = []\n",
        "        for i in range(len(flat_list)):\n",
        "            tmp = [s for s in sims if s.c1 == i or s.c2 == i]\n",
        "            tmp.sort(key=lambda x: x.value, reverse=True)\n",
        "            classes.append(tmp)\n",
        "\n",
        "        list_classes = []\n",
        "        for i in range(self.classes_number):\n",
        "            fx = math.ceil((len(classes[0]) - 1) / (self.classes_number - 1) * i)\n",
        "            cx = classes[0][int(fx)].c2  # fx can be float, cast to int\n",
        "            list_classes.append([cx])\n",
        "\n",
        "        m = 0\n",
        "        c = 1\n",
        "        limit = len(flat_list) - self.classes_number\n",
        "\n",
        "        while m < limit:\n",
        "            for i in range(self.classes_number):\n",
        "                j = list_classes[i][0]\n",
        "                cy = classes[j][c].c2 if classes[j][c].c1 in list_classes[i] else classes[j][c].c2\n",
        "\n",
        "                if not any(cy in sublist for sublist in list_classes):\n",
        "                    list_classes[i].append(cy)\n",
        "                    m += 1\n",
        "\n",
        "                if m >= limit:\n",
        "                  break\n",
        "            c += 1\n",
        "\n",
        "        class_index = 0\n",
        "        for cla in list_classes:\n",
        "            for x in cla:\n",
        "                self._add_category(f\"cat{class_index}\", flat_list[x])\n",
        "            class_index += 1\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training Start\")\n",
        "        self._race()\n",
        "        self._fit()\n",
        "\n",
        "        if not self.model_path:\n",
        "            self.model_path = f\"conquerer-{self.mode.lower()}-{self.word_pooling}-model.bin\"\n",
        "        self.save_model(self.model_path)\n",
        "        print(\"Training Finished\")\n",
        "\n",
        "    def _race(self):\n",
        "        percent = 0\n",
        "        all_combinations = sum(range(1, len(self.categories)))\n",
        "\n",
        "        for i in range(len(self.categories) - 1):\n",
        "            for j in range(i + 1, len(self.categories)):\n",
        "                c1, c2 = self._conquerer(self.categories[i], self.categories[j])\n",
        "                self.categories[i] = c1\n",
        "                self.categories[j] = c2\n",
        "\n",
        "                percent += 1\n",
        "                p = 100 * (percent + 1) / all_combinations\n",
        "\n",
        "                print(f\"Training {c1.name} -> {c2.name} {percent + 1}/{all_combinations} {math.ceil(p)}% {'#' * int(math.ceil(p / 4))}\", end='\\r') # \\r to overwrite the line\n",
        "        print() # add a newline after the loop\n",
        "\n",
        "    def _fit(self):\n",
        "        self.tensors.clear()\n",
        "        for category in self.categories:\n",
        "            for token in category.tokens:\n",
        "               #print(str(category.id) + \" \" + str(token.index) + \" \" + str(token.weight))\n",
        "                self.tensors.append(Tensor(category_id=category.id, word_id=token.index, weight=token.weight))\n",
        "\n",
        "    def _conquerer(self, c1, c2):\n",
        "        tokens = self._intersect(c1.tokens, c2.tokens)\n",
        "\n",
        "        for token in tokens:\n",
        "            index1 = next((i for i, t in enumerate(c1.tokens) if t.index == token.index), -1)\n",
        "            index2 = next((i for i, t in enumerate(c2.tokens) if t.index == token.index), -1)\n",
        "\n",
        "            if index1 > -1 and index2 > -1 and not token.nonce:\n",
        "                c1_alpha = self._alpha(token, tokens, c1)\n",
        "                c2_alpha = self._alpha(token, tokens, c2)\n",
        "\n",
        "                c1.tokens[index1].weight = (c1.tokens[index1].weight * c1_alpha) + (c1_alpha / (len(self.categories)+1))\n",
        "                c2.tokens[index2].weight = (c2.tokens[index2].weight * c2_alpha) + (c2_alpha / (len(self.categories)+1))\n",
        "\n",
        "        ntokens = self._non_intersect(c1.tokens, c2.tokens)\n",
        "\n",
        "        for token in ntokens:\n",
        "            index1 = next((i for i, t in enumerate(c1.tokens) if t.index == token.index), -1)\n",
        "            index2 = next((i for i, t in enumerate(c1.tokens) if t.index == token.index), -1)\n",
        "\n",
        "            if index1 > -1:\n",
        "                c1_alpha = self._alpha(token, tokens, c1)\n",
        "                c1.tokens[index1].weight += (c1_alpha / (len(self.categories)+1))\n",
        "            elif index2 > -1:\n",
        "                c2_alpha = self._alpha(token, tokens, c2)\n",
        "                c2.tokens[index2].weight += (c2_alpha / (len(self.categories)+1))\n",
        "\n",
        "        return c1, c2\n",
        "\n",
        "\n",
        "    def predict(self, text, max_results=2):\n",
        "        tokens, _ = Tokenize(self.word_pooling).apply(text, self.vocab)  # Assuming tokenize returns a tuple\n",
        "        results = []\n",
        "\n",
        "        for token in tokens:\n",
        "            ts = [t for t in self.tensors if t.word_id == token.index]\n",
        "            if ts:\n",
        "                for t in ts:\n",
        "                    category = next((c.name for c in self.categories if c.id == t.category_id), None)\n",
        "                    if category:\n",
        "                        index = next((i for i, r in enumerate(results) if r['category'] == category), -1)\n",
        "                        if index > -1:\n",
        "                            results[index]['category_count'] += 1\n",
        "                            results[index]['weight'] += (t.weight > self.dropout and t.weight or 0) + ( results[index]['category_count'] + len(tokens))   # Simplified conditional\n",
        "                        else:\n",
        "                            results.append({'category_id': t.category_id, 'category': category, 'weight': (t.weight > self.dropout and t.weight or 0), 'category_count': 1, 'input_token_count': len(tokens)})\n",
        "\n",
        "        results = self._softmax1(results)\n",
        "        results.sort(key=lambda r: r['confidence'], reverse=True)\n",
        "        return self._limiter(results, max_results)\n",
        "\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        print(f\"SaveModel: {filename} Tensors: {len(self.tensors)}\")\n",
        "\n",
        "        model = Model(tensors = self.tensors, vocab = self.vocab, categories = self._fit_categories_small(self.categories), vectors = self._fit_vectors_to_dict(self.vectors))\n",
        "\n",
        "        print(f\"Tensors: {len(self.tensors)}\")\n",
        "        print(f\"Vocab: {len(self.vocab)}\")\n",
        "        print(f\"Categories: {len(self.categories)}\")\n",
        "        print(f\"Vectors: {len(self.vectors)}\")\n",
        "\n",
        "        # Save as pickle (binary serialization, a replacement for protobuf)\n",
        "        # with open(filename, \"wb\") as f:\n",
        "        #    pickle.dump(model, f)\n",
        "        ProtobufConverter.save_to_file(model, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_json_model(self, filename):\n",
        "        options = {\"indent\": 4}  # For readable JSON\n",
        "\n",
        "        model = Model(tensors = self.tensors, vocab = self.vocab, categories = self._fit_categories_small(self.categories), vectors = self._fit_vectors_to_dict(self.vectors))\n",
        "\n",
        "        with open(f\"{filename}.json\", \"w\") as f:\n",
        "            json.dump(model, f, **options)\n",
        "\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        #with open(filename, \"rb\") as f:  # Load from pickle file\n",
        "        #    model = pickle.load(f)\n",
        "        model = ProtobufConverter.load_from_file(filename)\n",
        "\n",
        "        print(f\"Tensors: {len(model.tensors)}\")\n",
        "        print(f\"Vocab: {len(model.vocab)}\")\n",
        "        print(f\"Categories: {len(model.categories)}\")\n",
        "\n",
        "        return (model.tensors, model.vocab, model.categories,\n",
        "                self._fit_vectors_from_dict(model.vectors)) # Convert from dictionary\n",
        "\n",
        "\n",
        "    def _fit_vectors_to_dict(self, vectors):  # Helper function\n",
        "        word_vector = {\"embeddings\": []}\n",
        "        for vector in vectors:\n",
        "            embedding = {\"values\": vector} # Directly assign the list\n",
        "            word_vector[\"embeddings\"].append(embedding)\n",
        "        return word_vector\n",
        "\n",
        "\n",
        "    def _fit_vectors_from_dict(self, vectors_dict):  # Helper function\n",
        "        vectors = []\n",
        "        for embedding in vectors_dict.embeddings:\n",
        "            vectors.append(embedding.values)\n",
        "        return vectors\n",
        "\n",
        "\n",
        "    def _fit_categories_small(self, categories):  # Helper function\n",
        "        return [{\"id\": cat.id, \"name\": cat.name} for cat in categories]\n",
        "\n",
        "\n",
        "    def _fit_categories_from_small(self, categories_small) -> List[Category]:  # Helper function\n",
        "        return [Category(id=cat.id, name=cat.name) for cat in categories_small]\n",
        "\n",
        "\n",
        "    def _intersect(self, arr1, arr2):\n",
        "        result = []\n",
        "        for token1 in arr1:\n",
        "            for token2 in arr2:\n",
        "                if token1.index == token2.index and not token1.nonce:\n",
        "                    result.append(token1)\n",
        "        return result\n",
        "\n",
        "    def _non_intersect(self, arr1, arr2):\n",
        "        indices_lista2 = {t.index for t in arr2}\n",
        "        result = [t for t in arr1 if t.index not in indices_lista2] + [t for t in arr2 if not any(t1.index == t.index for t1 in arr1)]\n",
        "        return result\n",
        "\n",
        "    def _alpha(self, selected_token, intersect_tokens, cat):\n",
        "        _alpha = 0.0\n",
        "        position = selected_token.position\n",
        "\n",
        "        for token in intersect_tokens:\n",
        "            if token.index != selected_token.index:\n",
        "                index_cat = next((i for i, t in enumerate(cat.tokens) if t.index == token.index), -1)\n",
        "                if index_cat == -1:\n",
        "                    continue\n",
        "\n",
        "                near_position = self._alpha_minor_distance_position(position, cat.tokens[index_cat].position)\n",
        "                near_distance = abs(near_position - index_cat)\n",
        "                if near_distance == 0:\n",
        "                    near_distance = 10000\n",
        "                _alpha += 1.0 / (near_distance ** self.distance_temperature)\n",
        "        return _alpha / (len(intersect_tokens) + 1)\n",
        "\n",
        "\n",
        "    def _alpha_minor_distance_position(self, position, ids):\n",
        "        dist = 10000\n",
        "        for pos in position:\n",
        "            for id in ids:\n",
        "                dst = abs(pos - id)\n",
        "                if dst < dist:\n",
        "                    dist = dst\n",
        "        return dist\n",
        "\n",
        "    def _calc_ratio(self, c1, c2, t_count, v_count):\n",
        "        r = 2\n",
        "        if c1 > c2:\n",
        "            return (r * (c1 / c2)) / math.sqrt(t_count / v_count)\n",
        "        else:\n",
        "            return (r * (c1 / c2)) / math.sqrt(t_count / v_count)\n",
        "\n",
        "    def _softmax1(self, results):\n",
        "        sum_exp = sum(math.exp(result['weight']) for result in results)\n",
        "        for result in results:\n",
        "            result['exponential'] = math.exp(result['weight'])\n",
        "            result['confidence'] = result['exponential'] / sum_exp\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _softmax(self, results):\n",
        "        total_weight = sum(result['weight'] for result in results)\n",
        "        for result in results:\n",
        "            result['confidence'] = result['weight'] / total_weight\n",
        "        return results\n",
        "\n",
        "    def _limiter(self, results, max_results):\n",
        "        if len(results) > max_results:\n",
        "            del results[max_results:]  # Efficiently remove excess elements\n",
        "        return results\n",
        "\n",
        "    def export(self):\n",
        "        return self.categories, self.tensors, self.vocab\n",
        "\n",
        "    def _intersect_count(self, t1, t2): # Renamed for clarity\n",
        "        count = 0\n",
        "        for word1 in t1:\n",
        "            for word2 in t2:  # Corrected loop to iterate over t2\n",
        "                if word1 == word2:\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    def _similarity(self, t1, t2):\n",
        "        t1_arr = self._sanitize(t1).split()  # Split on whitespace by default\n",
        "        t2_arr = self._sanitize(t2).split()\n",
        "\n",
        "        intersect = self._intersect_count(t1_arr, t2_arr)\n",
        "        return (2.0 * intersect) / (len(t1_arr) + len(t2_arr))\n",
        "\n",
        "    def _sanitize(self, text):\n",
        "        text = text.lower()\n",
        "        for char in \",.*:'?!\":  # More efficient replacement\n",
        "            text = text.replace(char, \"\")\n",
        "        return text.replace(\"  \", \" \")  # Keep this replacement\n",
        "\n",
        "    def none_category(self, c, a, b, w):\n",
        "        r = c / (a + b)  # Simplified division\n",
        "        return r > c * self.non_category_threshold\n",
        "\n",
        "    def print_results(self, results):  # More descriptive name\n",
        "        for result in results:\n",
        "            t = sum(1 for tensor in self.tensors if tensor.category_id == result['category_id'])\n",
        "            print(f\"{result['category_id']} {result['category']} {result['confidence']} {t} {result['category_count']} {result['input_token_count']} {result['weight']} {self.none_category(t, result['category_count'], result['input_token_count'], result['weight'])}\")\n",
        "\n",
        "    def print_embeddings(self, text):\n",
        "        tokens = Tokenize.instance(self.word_pooling).apply(text, self.vocab)[0]\n",
        "\n",
        "        for token in tokens:\n",
        "            tensor = max((t for t in self.tensors if t.word_id == token.index), key=lambda t: t.weight, default=None)\n",
        "\n",
        "            if tensor:\n",
        "                category_name = next((c.name for c in self.categories if c.id == tensor.category_id), \"None Category\")\n",
        "                print(f\"{self.vocab[token.index].word} {category_name} {tensor.weight}\")\n",
        "            else:\n",
        "                print(f\"{self.vocab[token.index].word} None Category\")\n",
        "\n",
        "\n",
        "    def print_vocab(self, vocab_list=None): # Added optional parameter\n",
        "        vocab_list = vocab_list or self.vocab # Use self.vocab if no list is provided\n",
        "        for vocab_item in vocab_list:\n",
        "            print(f\"{vocab_item.word} {vocab_item.word_partial}\")\n",
        "\n",
        "    def print_tensors(self, tensor_list=None): # Added optional parameter\n",
        "        tensor_list = tensor_list or self.tensors # Use self.tensors if no list is provided\n",
        "        for tensor in tensor_list:\n",
        "            print(f\"{tensor.word_id} {tensor.category_id}\")\n",
        "\n",
        "    def print_category_tokens(self, category):\n",
        "        print(f\"{category.name} -----------------------------------------------------\")\n",
        "        for i, token in enumerate(category.tokens):\n",
        "            print(f\"{i} {self.vocab[token.index].word} {token.count}\")\n",
        "\n",
        "    def print_vectors(self, vectors_list):\n",
        "        for vector in vectors_list:\n",
        "            print(\", \".join(map(str, vector)))  # More concise printing\n",
        "\n",
        "    def print_categories(self):\n",
        "        for category in self.categories:\n",
        "            print(f\"{category.id} {category.name}\")"
      ],
      "metadata": {
        "id": "gOgc7C45bC-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokz = Tokenize(word_pooling=1)\n",
        "\n",
        "\n",
        "def Training():\n",
        "    list1 = [\n",
        "        \"a shigelose é uma infecção intestinal causada por bactérias do gênero shigella. a espécie shigella dysenteriae é considerada a mais grave e é capaz de causar diarreia com sangue e mucosas, além de febre, dor abdominal e náuseas. a transmissão pode ocorrer por água ou alimentos contaminados, além do contato direto com fezes infectadas.\",\n",
        "        \"a febre paratifóide b é uma infecção bacteriana aguda causada pela bactéria salmonella paratyphi b.\",\n",
        "        \"essa categoria inclui outras intoxicações alimentares bacterianas não especificadas, exceto aquelas causadas por salmonella, shigella, campylobacter e clostridium perfringens\",\n",
        "        \"doença parasitária causada pelo protozoário giardia lamblia, que afeta principalmente o trato gastrointestinal humano.\",\n",
        "        \"doença infecciosa causada pela bactéria mycobacterium tuberculosis, que afeta principalmente os pulmões e pode ser transmitida pelo ar.\"\n",
        "    ]\n",
        "\n",
        "    list2 = [\n",
        "        \"doenças causadas pela exposição a poluentes presentes no solo, como metais pesados e substâncias químicas tóxicas.\",\n",
        "        \"a exposição ocupacional a agentes tóxicos em outras indústrias pode ocorrer em diversos setores, como em trabalhadores que lidam com produtos químicos, metais pesados, solventes e outras substâncias tóxicas. essa exposição pode causar problemas neurológicos, respiratórios, dermatológicos e até mesmo câncer.\",\n",
        "        \"essa categoria inclui doenças e transtornos relacionados ao trabalho que não se enquadram em outras categorias específicas. alguns exemplos incluem dores musculares e articulares, fadiga, ansiedade e estresse ocupacional.\",\n",
        "        \"problema relacionado com o medo ou possibilidade de perder o emprego atual, o que pode levar a ansiedade, estresse e outros problemas emocionais\",\n",
        "        \"esta categoria inclui problemas relacionados com a educação e a alfabetização que não se enquadram nas categorias anteriores, como dificuldades de aprendizagem, transtornos de déficit de atenção e hiperatividade, entre outros.\",\n",
        "        \"analfabetismo é a incapacidade de ler e escrever, enquanto baixa escolaridade se refere a indivíduos que possuem pouca escolaridade formal. ambas são consideradas fatores de risco para diversas doenças e problemas de saúde, como a falta de acesso à informação e dificuldade em compreender informações importantes sobre saúde.\"\n",
        "    ]\n",
        "\n",
        "    list3 = [\n",
        "        \"não foi encontrado dados precisos sobre a taxa de ocorrência e letalidade específicas para essa doença, mas pode-se considerar os dados gerais da doença diverticular do intestino grosso\",\n",
        "        \"a fissura anal aguda é uma lesão na pele que reveste o ânus e pode ser bastante dolorosa. ela ocorre principalmente em adultos jovens e saudáveis e é mais comum em mulheres do que em homens.\",\n",
        "        \"o volvo é uma condição rara e grave que ocorre quando uma porção do intestino se torce em torno de sua própria base, interrompendo o fluxo sanguíneo para essa área. isso pode levar à morte do tecido intestinal e à perfuração do intestino, o que pode levar a uma infecção grave.\",\n",
        "        \"a fístula anorretal é uma comunicação anormal que se forma entre o canal anal ou reto e a pele ao redor do ânus. a causa mais comum é uma infecção do trato anal ou reto, que pode levar a um abscesso que não foi tratado adequadamente. \",\n",
        "        \"o prolapso anal é uma condição em que a mucosa retal se projeta através do ânus, podendo ser classificado em três tipos: prolapso mucoso, prolapso retal completo e prolapso retal interno.\",\n",
        "        \"a proctite por radiação é uma inflamação da mucosa retal que ocorre como resultado da radioterapia pélvica. é comum em pacientes com câncer de próstata, câncer de reto ou outros tipos de câncer pélvico que são tratados com radioterapia.\"\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    vocab = Tokenize.instance(word_pooling=1).vocab([list1, list2, list3])\n",
        "    #Tokenize.instance().print_vocab(vocab)\n",
        "\n",
        "    clasf = Classify.instance()\n",
        "    clasf.vocab(vocab)\n",
        "    #clasf.word_pooling(1)\n",
        "    clasf.model(\"word-conquerer-attention-model.dat\")\n",
        "    clasf.add_category(\"CatA\", list1)\n",
        "    clasf.add_category(\"CatZ\", list2)\n",
        "    clasf.add_category(\"CatK\", list3)\n",
        "    clasf.train()\n",
        "\n",
        "\n",
        "Training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl1O7fiOcAc2",
        "outputId": "11f97b7b-f261-4605-e0d8-e0a69a6be1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Start\n",
            "Training CatA -> CatZ 2/3 67% #################\rTraining CatA -> CatK 3/3 100% #########################\rTraining CatZ -> CatK 4/3 134% ##################################\r\n",
            "SaveModel: word-conquerer-attention-model.dat Tensors: 333\n",
            "Tensors: 333\n",
            "Vocab: 272\n",
            "Categories: 3\n",
            "Vectors: 17\n",
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clasf = Classify.instance().model(\"word-conquerer-attention-model.dat\", True)\n",
        "r1 = clasf.predict(\"problema relacionado com o medo ou possibilidade de perder o emprego atual ansiedade, que afeta principalmente\", 3)\n",
        "clasf.print_results(r1)\n",
        "print()\n",
        "\n",
        "#clasf.print_embeddings(\"problema relacionado com o medo ou possibilidade de perder o emprego atual ansiedade, que afeta principalmente\")\n",
        "#print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QtgvREokwth",
        "outputId": "a0f668e7-6480-49df-fa9e-00e680cda32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensors: 333\n",
            "Vocab: 272\n",
            "Categories: 3\n",
            "2 CatZ 1.0 119 15 17 365.461753803121 False\n",
            "1 CatA 1.2150854493257377e-80 88 9 17 181.44976076658006 False\n",
            "3 CatK 2.0075382750554658e-92 126 8 17 154.32083448035593 False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generative:\n",
        "    def __init__(self, categories: None, tensors:  None, vocab: None):\n",
        "        self.word_pooling = 1.0\n",
        "        self.categories = categories if categories else []\n",
        "        self.tensors = tensors if tensors else []\n",
        "        self.vocab = vocab if vocab else []\n",
        "        self.vectors = []\n",
        "        self.model_path = \"\"\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def instance(categories = None, tensors = None, vocab =  None):\n",
        "        return Generative(categories, tensors, vocab)\n",
        "\n",
        "    def word_pooling(self, word_pooling: float):\n",
        "        self.word_pooling = word_pooling\n",
        "        return self\n",
        "\n",
        "    def model(self, model_path: str, read: bool = False):\n",
        "        print(f\"Model: {model_path} {os.path.exists(model_path)} {read}\")\n",
        "        if read and os.path.exists(model_path):\n",
        "            self.tensors, self.vocab, _categories, self.vectors = self.load_model(model_path)\n",
        "            self.categories = self._fit_categories_from_small(_categories)\n",
        "        return self\n",
        "\n",
        "    def next_word(self, word: str, useContext: bool = False):\n",
        "        word_id = next((i for i, v in enumerate(self.vocab) if v.word == word), -1)\n",
        "        _vectors = self.vectors\n",
        "\n",
        "        if useContext:\n",
        "            classifier = Classify.instance(\"\", self.vocab, self.categories, self.tensors, self.vectors)\n",
        "            results = classifier.predict(word, 1)\n",
        "            t = sum(1 for c in self.tensors if c.category_id == results[0][\"category_id\"])\n",
        "\n",
        "            if not classifier.none_category(t, results[0][\"category_count\"], results[0][\"input_token_count\"], results[0][\"weight\"]):\n",
        "                _vectors = [v for v in self.vectors if v[0] == results[0][\"category_id\"]]\n",
        "\n",
        "        for lst in _vectors:\n",
        "            index = lst.index(word_id) if word_id in lst else -1\n",
        "            if index > -1:\n",
        "                for i in range(index, len(lst) - index):\n",
        "                    if self.vocab[lst[i]].word == \"<end>\":\n",
        "                        print()\n",
        "                        break\n",
        "                    print(f\"{self.vocab[lst[i]].word} \", end=\"\")\n",
        "\n",
        "\n",
        "    def qna(self, phrase: str, useContext: bool = False):\n",
        "        print(f\"{self.capitalizar_primeira_letra(phrase)} \", end=\"\")\n",
        "        _vectors = self.vectors\n",
        "\n",
        "        if useContext:\n",
        "            classifier = Classify.instance(\"\",self.vocab, self.categories, self.tensors, self.vectors)\n",
        "            results = classifier.predict(phrase, 1)\n",
        "            classifier.print_results(results)\n",
        "            t = sum(1 for c in self.tensors if c.category_id == results[0][\"category_id\"])\n",
        "            if not classifier.none_category(t, results[0][\"category_count\"], results[0][\"input_token_count\"], results[0][\"weight\"]) or results[0][\"confidence\"] > 0.5:\n",
        "                _vectors = [v for v in self.vectors if v[0] == results[0][\"category_id\"]]\n",
        "\n",
        "        _vec_db = []\n",
        "        words = self.pontuation(phrase).lower().split()\n",
        "        words_ids = [next((i for i, v in enumerate(self.vocab) if v.word == word), -1) for word in words]\n",
        "\n",
        "\n",
        "\n",
        "        for word_id in words_ids:\n",
        "            at = [t for t in self.tensors if t.word_id == word_id]\n",
        "            at.sort(key=lambda t: t.weight, reverse=True)\n",
        "            w = at[0].weight if at else 0\n",
        "\n",
        "            if w > 0:\n",
        "                for i, vec in enumerate(_vectors):\n",
        "                    index = vec.index(word_id) if word_id in vec else -1\n",
        "                    if index > -1:\n",
        "                        v_index = next((j for j, v in enumerate(_vec_db) if v.index == i), -1)\n",
        "                        if v_index > -1:\n",
        "                            _vec_db[v_index].count += 1\n",
        "                            _vec_db[v_index].weight += self.beta_distance(vec, words_ids, word_id)\n",
        "                        else:\n",
        "                            _vec_db.append(GenVec(index=i, count=1, weight=w))\n",
        "\n",
        "        _vec_db.sort(key=lambda v: v.weight, reverse=True)\n",
        "        nextCaption = True\n",
        "\n",
        "        for i in range(1, len(_vectors[_vec_db[0].index])):\n",
        "            word = self.vocab[_vectors[_vec_db[0].index][i]].word\n",
        "            if nextCaption and word != \"<end>\":\n",
        "                word = word.title()\n",
        "\n",
        "            if word == \".\":\n",
        "                nextCaption = True\n",
        "            else:\n",
        "                nextCaption = False\n",
        "\n",
        "            if word == \"<end>\":\n",
        "                break\n",
        "            print(f\"{word} \", end=\"\", flush=True)\n",
        "            time.sleep(0.15)\n",
        "\n",
        "        print()\n",
        "\n",
        "\n",
        "    def analyze(self, phrase: str, useContext: bool = False):\n",
        "        print(f\"{self.capitalizar_primeira_letra(phrase)} \", end=\"\")\n",
        "        _vectors = self.vectors\n",
        "\n",
        "        if useContext:\n",
        "            classifier = Classify.instance(self.categories, self.tensors, self.vocab, self.vectors)\n",
        "            results = classifier.predict(phrase, 1)\n",
        "            t = sum(1 for c in self.tensors if c.category_id == results[0].category_id)\n",
        "\n",
        "            if not classifier.none_category(t, results[0].category_count, results[0].input_token_count, results[0].weight) or results[0].weight > 0.5:\n",
        "                _vectors = [v for v in self.vectors if v[0] == results[0].category_id]\n",
        "\n",
        "        words = self.pontuation(phrase).lower().split()\n",
        "        for word in words:\n",
        "            print(f\"{word} \")\n",
        "            word_id = next((i for i, v in enumerate(self.vocab) if v.word == word), -1)\n",
        "            _tensors = [t for t in self.tensors if t.word_id == word_id]\n",
        "            _tensors.sort(key=lambda t: t.weight, reverse=True)\n",
        "\n",
        "            for t in _tensors:\n",
        "                category = next(c for c in self.categories if c.id == t.category_id)\n",
        "                print(f\"\\t {category.name} {t.weight}\")\n",
        "\n",
        "\n",
        "    def beta_distance(self, lst: List[int], words_ids: List[int], word_id: int) -> float:\n",
        "        d = 1000000\n",
        "        p_word = lst.index(word_id) if word_id in lst else -1\n",
        "\n",
        "        if p_word > -1:\n",
        "            for id in words_ids:\n",
        "                if id != word_id:\n",
        "                    p_id = lst.index(id) if id in lst else -1\n",
        "                    if p_id > -1:\n",
        "                        dist = abs(p_id - p_word)\n",
        "                        if dist < d:\n",
        "                            d = dist\n",
        "\n",
        "        return 1 / d\n",
        "\n",
        "\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        #with open(filename, \"rb\") as f:  # Load from pickle file\n",
        "        #    model = pickle.load(f)\n",
        "        model = ProtobufConverter.load_from_file(filename)\n",
        "\n",
        "        print(f\"Tensors: {len(model.tensors)}\")\n",
        "        print(f\"Vocab: {len(model.vocab)}\")\n",
        "        print(f\"Categories: {len(model.categories)}\")\n",
        "\n",
        "        return (model.tensors, model.vocab, model.categories,\n",
        "                self._fit_vectors_from_dict(model.vectors)) # Convert from dictionary\n",
        "\n",
        "\n",
        "    def _fit_vectors(self, vectors: 'WordVector') -> List[List[int]]:\n",
        "        return [list(emb.values) for emb in vectors.embeddings]\n",
        "\n",
        "    def _fit_vectors_from_dict(self, vectors_dict):  # Helper function\n",
        "        vectors = []\n",
        "        for embedding in vectors_dict.embeddings:\n",
        "            vectors.append(embedding.values)\n",
        "        return vectors\n",
        "\n",
        "    def _fit_categories_from_small(self, categories_small) -> List[Category]:  # Helper function\n",
        "        return [Category(id=cat.id, name=cat.name) for cat in categories_small]\n",
        "\n",
        "\n",
        "    def vocab(self) -> List['Vocab']:\n",
        "        return self.vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def word_pooling(word: str, rate: float) -> str:\n",
        "        length = len(word)\n",
        "        if length <= 4:\n",
        "            return word\n",
        "        pad = int(np.ceil(length * rate))\n",
        "        return word[:pad]\n",
        "\n",
        "    def pontuation(self, text: str) -> str:\n",
        "        text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"*\", \"\").replace(\":\", \"\").replace(\"'\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"  \", \" \")\n",
        "        return text\n",
        "\n",
        "    def pontuation_r(self, text: str) -> str:\n",
        "        text = text.lower().replace(\" , \", \",\").replace(\" . \", \".\").replace(\" * \", \"*\").replace(\" : \", \":\").replace(\" ' \", \"'\")\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def print_vectors(self, lst: List[List[int]]):\n",
        "        for vector in lst:\n",
        "            print(\", \".join(map(str, vector)))\n",
        "\n",
        "    def print_gen_vec(self, lst: List['GenVec']):\n",
        "        for genVec in lst:\n",
        "            print(f\"{genVec.index} {genVec.count} {genVec.weight} {genVec.count * genVec.weight}\")\n",
        "\n",
        "    def print_phrase(self, lst: List[int]):\n",
        "        for i in range(1, len(lst)):\n",
        "            print(f\"{self.vocab[lst[i]].word} \", end=\"\")\n",
        "        print(\"\\n----------------------------------------------------------------\")\n",
        "\n",
        "    @staticmethod\n",
        "    def capitalizar_primeira_letra(input: str) -> str:\n",
        "        if not input:\n",
        "            return input\n",
        "        return input[0].upper() + input[1:]"
      ],
      "metadata": {
        "id": "ELNzqUgou0ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai = Generative.instance().model(\"cid10-1wp-model-2025-05-23.dat\", True)\n",
        "genai.qna(\"a neoplasia maligna da glote é um tipo de câncer\", True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li50QXxovOkK",
        "outputId": "2a61ac28-c610-488f-d310-6f2402b61e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: cid10-1wp-model-2025-05-23.dat True True\n",
            "Tensors: 35897\n",
            "Vocab: 11763\n",
            "Categories: 22\n",
            "A neoplasia maligna da glote é um tipo de câncer 3 C 0.9999999997048128 1656 11 11 176.0727178293969 False\n",
            "A neoplasia maligna da glote é um tipo de câncer que afeta as pregas vocais na laringe , sendo mais comum em homens acima de 50 anos . Os principais fatores de risco incluem tabagismo e consumo excessivo de álcool . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq:\n",
        "    def __init__(self, model_path = None, vocab = None, categories = None, tensors = None, vectors = None):\n",
        "        self.word_pooling = 1.0\n",
        "        self.distance_temperature = 1.0\n",
        "        self.model_path = model_path if model_path else \"\"\n",
        "        if vocab != None: self.vocab = vocab or []\n",
        "        self.categories = categories if categories else []\n",
        "        self.tensors = tensors if tensors else []\n",
        "        self.vectors = vectors if vectors else []\n",
        "\n",
        "    @staticmethod\n",
        "    def instance(model_path = None, vocab = None, categories = None, tensors = None, vectors = None):\n",
        "        return Seq2Seq(model_path, vocab, categories, tensors, vectors)\n",
        "\n",
        "    def word_pooling(self, word_pooling: float):\n",
        "        self.word_pooling = word_pooling\n",
        "        return self\n",
        "\n",
        "    def vocab(self, vocab: List['Vocab']):\n",
        "        self.vocab = vocab\n",
        "        return self\n",
        "\n",
        "    def distance_temperature(self, distance_temperature):\n",
        "        self.distance_temperature = distance_temperature\n",
        "        return self\n",
        "\n",
        "    def model(self, model_path: str, read: bool = False):\n",
        "        self.model_path = model_path\n",
        "        print(f\"Model: {model_path} {os.path.exists(model_path)} {read}\")\n",
        "        if read and os.path.exists(model_path):\n",
        "            self.tensors, self.vocab, _categories, self.vectors = self.load_model(model_path)\n",
        "            self.categories = self._fit_categories_from_small(_categories)\n",
        "        return self\n",
        "\n",
        "    def add_category(self, name: str, question: str, answer: str):\n",
        "        index = next((i for i, c in enumerate(self.categories) if c.name == name), -1)\n",
        "\n",
        "        if index == -1:\n",
        "            id = len(self.categories) + 1\n",
        "            t = Category(id=id, name=name)\n",
        "            t.add_tokens(Tokenize.instance(self.word_pooling).apply(question, self.vocab))\n",
        "            self.categories.append(t)\n",
        "            ids = Tokenize.instance(self.word_pooling).embedding(id, answer, self.vocab)\n",
        "            self.vectors.append(ids)\n",
        "        else:\n",
        "            self.categories[index].add_tokens(Tokenize.instance(self.word_pooling).apply(question, self.vocab))\n",
        "            ids = Tokenize.instance(self.word_pooling).embedding(self.categories[index].id, answer, self.vocab)\n",
        "            self.vectors.append(ids)\n",
        "        return self\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training Start\")\n",
        "        self.race()\n",
        "        self.fit()\n",
        "\n",
        "        if not self.model_path:\n",
        "            self.model_path = f\"conquerer-{self.word_pooling}-model.bin\"\n",
        "        self.save_model(self.model_path)\n",
        "        print(\"Training Finished\")\n",
        "\n",
        "    def race(self):\n",
        "        percent = 0\n",
        "        all = sum(range(1, len(self.categories)))\n",
        "\n",
        "        for i in range(len(self.categories) - 1):\n",
        "            for j in range(i + 1, len(self.categories)):\n",
        "                c1, c2 = self.conquerer(self.categories[i], self.categories[j])\n",
        "                self.categories[i] = c1\n",
        "                self.categories[j] = c2\n",
        "                percent += 1\n",
        "                p = (100 * (percent + 1) / all)\n",
        "                print(f\"Training {c1.name} -> {c2.name} {percent + 1}/{all} {math.ceil(p)}% \", end=\"\")\n",
        "                print(\"#\" * int(math.ceil(p / 4)), end=\"\")\n",
        "                print()\n",
        "\n",
        "    def fit(self):\n",
        "        self.tensors.clear()\n",
        "        for category in self.categories:\n",
        "            for token in category.tokens:\n",
        "                self.tensors.append(Tensor(category_id=category.id, word_id=token.index, weight=token.weight))\n",
        "\n",
        "    def conquerer(self, c1: Category, c2: Category) -> Tuple[Category, Category]:\n",
        "        tokens = self.intersect(c1.tokens, c2.tokens)\n",
        "        for token in tokens:\n",
        "            index1 = next((i for i, t in enumerate(c1.tokens) if t.index == token.index), -1)\n",
        "            index2 = next((i for i, t in enumerate(c2.tokens) if t.index == token.index), -1)\n",
        "            if index1 > -1 and index2 > -1 and not token.nonce:\n",
        "                c1Alpha = self.alpha(token, tokens, c1)\n",
        "                c2Alpha = self.alpha(token, tokens, c2)\n",
        "\n",
        "                c1.tokens[index1].weight = (c1.tokens[index1].weight * c1Alpha) + (c1Alpha / (len(self.categories)+1))\n",
        "                c2.tokens[index2].weight = (c2.tokens[index2].weight * c2Alpha) + (c2Alpha / (len(self.categories)+1))\n",
        "        ntokens = self.non_intersect(c1.tokens, c2.tokens)\n",
        "        for token in ntokens:\n",
        "            index1 = next((i for i, t in enumerate(c1.tokens) if t.index == token.index), -1)\n",
        "            index2 = next((i for i, t in enumerate(c2.tokens) if t.index == token.index), -1)\n",
        "            if index1 > -1:\n",
        "                c1Alpha = self.alpha(token, tokens, c1)\n",
        "                c1.tokens[index1].weight += (c1Alpha / (len(self.categories)+1))\n",
        "            elif index2 > -1:\n",
        "                c2Alpha = self.alpha(token, tokens, c2)\n",
        "                c2.tokens[index2].weight += (c2Alpha / (len(self.categories)+1))\n",
        "        return c1, c2\n",
        "\n",
        "    def predict(self, phrase: str, threshold: float = 0.4):\n",
        "        print(f\"{self.capitalizar_primeira_letra(phrase)} \", end=\"\")\n",
        "        print()\n",
        "        _vectors = self.vectors\n",
        "\n",
        "        classifier = Classify.instance(\"\", self.vocab,self.categories, self.tensors, self.vectors)\n",
        "        results = [r for r in classifier.predict(phrase, 1) if r[\"confidence\"] > threshold]\n",
        "\n",
        "        if(len(results) == 0):\n",
        "            print(\"Não fui treinado com esse conhecimento!\")\n",
        "            return\n",
        "\n",
        "        index = int(results[0][\"category\"])\n",
        "\n",
        "        nextCaption = True\n",
        "        for i in range(1, len(_vectors[index])):\n",
        "            word = self.vocab[_vectors[index][i]].word\n",
        "            if nextCaption and word != \"<end>\":\n",
        "                word = word.title()\n",
        "            if word == \".\":\n",
        "                nextCaption = True\n",
        "            else:\n",
        "                nextCaption = False\n",
        "            if word == \"<end>\":\n",
        "                break\n",
        "            print(f\"{word} \", end=\"\", flush=True)\n",
        "            time.sleep(0.15)\n",
        "        print()\n",
        "\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        print(f\"SaveModel: {filename} Tensors: {len(self.tensors)}\")\n",
        "\n",
        "        model = Model(tensors = self.tensors, vocab = self.vocab, categories = self._fit_categories_small(self.categories), vectors = self._fit_vectors_to_dict(self.vectors))\n",
        "\n",
        "        print(f\"Tensors: {len(self.tensors)}\")\n",
        "        print(f\"Vocab: {len(self.vocab)}\")\n",
        "        print(f\"Categories: {len(self.categories)}\")\n",
        "        print(f\"Vectors: {len(self.vectors)}\")\n",
        "\n",
        "        # Save as pickle (binary serialization, a replacement for protobuf)\n",
        "        # with open(filename, \"wb\") as f:\n",
        "        #    pickle.dump(model, f)\n",
        "        ProtobufConverter.save_to_file(model, filename)\n",
        "\n",
        "\n",
        "    def save_json_model(self, filename):\n",
        "        options = {\"indent\": 4}  # For readable JSON\n",
        "\n",
        "        model = {\n",
        "            \"tensors\": self.tensors,\n",
        "            \"vocab\": self.vocab,\n",
        "            \"categories\": self._fit_categories_small(self.categories),\n",
        "            \"vectors\": self._fit_vectors_to_dict(self.vectors)\n",
        "        }\n",
        "\n",
        "        with open(f\"{filename}.json\", \"w\") as f:\n",
        "            json.dump(model, f, **options)\n",
        "\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        #with open(filename, \"rb\") as f:  # Load from pickle file\n",
        "        #    model = pickle.load(f)\n",
        "        model = ProtobufConverter.load_from_file(filename)\n",
        "\n",
        "        print(f\"Tensors: {len(model.tensors)}\")\n",
        "        print(f\"Vocab: {len(model.vocab)}\")\n",
        "        print(f\"Categories: {len(model.categories)}\")\n",
        "\n",
        "        return (model.tensors, model.vocab, model.categories,\n",
        "                self._fit_vectors_from_dict(model.vectors)) # Convert from dictionary\n",
        "\n",
        "\n",
        "    def _fit_vectors_to_dict(self, vectors):  # Helper function\n",
        "        word_vector = {\"embeddings\": []}\n",
        "        for vector in vectors:\n",
        "            embedding = {\"values\": vector} # Directly assign the list\n",
        "            word_vector[\"embeddings\"].append(embedding)\n",
        "        return word_vector\n",
        "\n",
        "\n",
        "    def _fit_vectors_from_dict(self, vectors_dict):  # Helper function\n",
        "        vectors = []\n",
        "        for embedding in vectors_dict.embeddings:\n",
        "            vectors.append(embedding.values)\n",
        "        return vectors\n",
        "\n",
        "    def _fit_categories_small(self, categories):  # Helper function\n",
        "        return [{\"id\": cat.id, \"name\": cat.name} for cat in categories]\n",
        "\n",
        "\n",
        "    def _fit_categories_from_small(self, categories_small) -> List[Category]:  # Helper function\n",
        "        return [Category(id=cat.id, name=cat.name) for cat in categories_small]\n",
        "\n",
        "    def intersect(self, arr1: List['Token'], arr2: List['Token']) -> List['Token']:\n",
        "        return [t1 for t1 in arr1 for t2 in arr2 if t1.index == t2.index and not t1.nonce]\n",
        "\n",
        "    def non_intersect(self, arr1: List['Token'], arr2: List['Token']) -> List['Token']:\n",
        "        indices_lista2 = {t.index for t in arr2}\n",
        "        return [t for t in arr1 if t.index not in indices_lista2] + [t for t in arr2 if not any(t1.index == t.index for t1 in arr1)]\n",
        "\n",
        "    def alpha(self, selected_token: 'Token', intersect_tokens: List['Token'], cat: 'Category') -> float:\n",
        "        _alpha = 0.0\n",
        "        position = selected_token.position\n",
        "        for token in intersect_tokens:\n",
        "            if token.index != selected_token.index:\n",
        "                index_cat = next((i for i, t in enumerate(cat.tokens) if t.index == token.index), -1)\n",
        "                if index_cat == -1:\n",
        "                    continue\n",
        "                near_position = self.alpha_minor_distance_position(position, cat.tokens[index_cat].position)\n",
        "                near_distance = abs(near_position - index_cat)\n",
        "                if near_distance == 0:\n",
        "                    near_distance = 10000\n",
        "                _alpha += 1.0 / (near_distance ** self.distance_temperature)\n",
        "        return _alpha / (len(intersect_tokens) + 1)\n",
        "\n",
        "    def alpha_minor_distance_position(self, position: List[int], ids: List[int]) -> int:\n",
        "        dist = 10000\n",
        "        rid = 0\n",
        "        for pos in position:\n",
        "            for id in ids:\n",
        "                dst = abs(pos - id)\n",
        "                if dst < dist:\n",
        "                    dist = dst\n",
        "                    rid = id\n",
        "        return dist\n",
        "\n",
        "    def pontuation(self, text: str) -> str:\n",
        "        text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"*\", \"\").replace(\":\", \"\").replace(\"'\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"  \", \" \")\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def capitalizar_primeira_letra(input: str) -> str:\n",
        "        if not input:\n",
        "            return input\n",
        "        return input[0].upper() + input[1:]"
      ],
      "metadata": {
        "id": "YSpRkDjMvk7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list4 = [\n",
        "    [\"Qual é o maior planeta do sistema solar?\", \"Júpiter\"],\n",
        "    [\"Quem pintou a Mona Lisa?\", \"Leonardo da Vinci\"],\n",
        "    [\"Qual é a capital do Japão?\", \"Tóquio\"],\n",
        "    [\"Quem escreveu 'Dom Quixote'?\", \"Miguel de Cervantes\"],\n",
        "    [\"Quantos ossos tem o corpo humano adulto?\", \"206\"],\n",
        "    [\"Qual é o elemento químico representado pelo símbolo 'O'?\", \"Oxigênio\"],\n",
        "    [\"Quem foi o primeiro homem a pisar na Lua?\", \"Neil Armstrong\"],\n",
        "    [\"Qual é a moeda oficial do Reino Unido?\", \"Libra esterlina\"],\n",
        "    [\"Qual é o maior oceano do mundo?\", \"Oceano Pacífico\"],\n",
        "    [\"Quantos lados tem um hexágono?\", \"Seis\"],\n",
        "    [\"Quem descobriu o Brasil?\", \"Pedro Álvares Cabral\"],\n",
        "    [\"Qual é a montanha mais alta do mundo?\", \"Monte Everest\"],\n",
        "    [\"Em que ano começou a Primeira Guerra Mundial?\", \"1914\"],\n",
        "    [\"Qual é a capital da França?\", \"Paris\"],\n",
        "    [\"Quem desenvolveu a Teoria da Relatividade?\", \"Albert Einstein\"],\n",
        "    [\"Qual é o metal líquido à temperatura ambiente?\", \"Mercúrio\"],\n",
        "    [\"Quantos estados tem o Brasil?\", \"26 estados e um Distrito Federal\"],\n",
        "    [\"Quem foi o primeiro presidente do Brasil?\", \"Deodoro da Fonseca\"],\n",
        "    [\"Qual é o animal terrestre mais rápido?\", \"Guepardo\"],\n",
        "    [\"Quem é o autor de 'Romeu e Julieta'?\", \"William Shakespeare\"],\n",
        "    [\"Qual é a fórmula química da água?\", \"H₂O\"],\n",
        "    [\"Quem inventou a lâmpada elétrica?\", \"Thomas Edison\"],\n",
        "    [\"Qual é o maior deserto do mundo?\", \"Deserto da Antártida\"],\n",
        "    [\"Qual país é famoso pela Torre Eiffel?\", \"França\"],\n",
        "    [\"Quem criou a teoria da evolução das espécies?\", \"Charles Darwin\"],\n",
        "    [\"Quantos planetas existem no sistema solar?\", \"Oito\"],\n",
        "    [\"Qual é o nome do maior rio do mundo em volume de água?\", \"Rio Amazonas\"],\n",
        "    [\"Qual é o nome da maior floresta tropical do mundo?\", \"Amazônia\"],\n",
        "    [\"Quem escreveu 'O Pequeno Príncipe'?\", \"Antoine de Saint-Exupéry\"],\n",
        "    [\"Em que ano terminou a Segunda Guerra Mundial?\", \"1945\"],\n",
        "    [\"Qual é o órgão responsável por bombear sangue no corpo humano?\", \"Coração\"],\n",
        "    [\"Qual é a capital da Austrália?\", \"Camberra\"],\n",
        "    [\"Quem foi o líder da Revolução Russa de 1917?\", \"Vladimir Lênin\"],\n",
        "    [\"Qual é o nome do gás essencial para a respiração humana?\", \"Oxigênio\"],\n",
        "    [\"Quem pintou 'A Última Ceia'?\", \"Leonardo da Vinci\"],\n",
        "    [\"Qual é o país com a maior população do mundo?\", \"China\"],\n",
        "    [\"Quem escreveu 'A Divina Comédia'?\", \"Dante Alighieri\"],\n",
        "    [\"Qual é o nome do processo em que as plantas produzem seu próprio alimento?\", \"Fotossíntese\"],\n",
        "    [\"Qual é o maior mamífero do mundo?\", \"Baleia-azul\"],\n",
        "    [\"Quem foi o fundador do Império Mongol?\", \"Genghis Khan\"]\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "vocab = Tokenize.instance(word_pooling=1).vocab(list4)\n",
        "\n",
        "model = Seq2Seq.instance()\n",
        "model.vocab(vocab)\n",
        "model.model(\"seq2seq-model.dat\")\n",
        "\n",
        "\n",
        "for i in range(len(list4)):\n",
        "   model.add_category(str(i), list4[i][0], list4[i][1])\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "id": "T718Zt6wvpcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq.instance().model(\"seq2seq-model.dat\", True)\n",
        "model.predict(\"Qual é o maior mamífero do mundo?\", 0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlB7YeDzvyIT",
        "outputId": "8df95c31-8351-4d3c-a310-a73a39185557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: seq2seq-model.dat True True\n",
            "Tensors: 340\n",
            "Vocab: 193\n",
            "Categories: 40\n",
            "Qual é o maior mamífero do mundo? \n",
            "Baleia-Azul \n"
          ]
        }
      ]
    }
  ]
}